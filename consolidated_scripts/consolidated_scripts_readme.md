Health Care Twitter Analysis Public Files Documentation

Edited By: Longbo Qiao
Email: longboqiao@gmail.com
Date:  6/28/2013

#### Installation
-	Python 2.7.5 is highly recommended.
-	Python libraries you will need(you may need more than this list): ntlk,nose,numpy,oauth,oauth2,scikit_learn,scipy-textmining,tweepy
-	‘Dictionaries’ folder contains all the dictionaries/word list/configuration file

#### Description of Files

1.	AFINN-111.txt
	A list of 2477 words and phrases with sentiment scores that defined by Technical University of Denmark. [http://www2.imm.dtu.dk/pubdb/views/publication_details.php?id=6010][1]

2.	config.json
Configuration file that used by __twitterstream.py__ to fetch tweets

3.	newWL.csv
Wordlist that generated by __Generate_newWL.py __for classification

4.	stopwords.csv
A group of less value words in medical context. Has been extracted and modified based on R’s ‘stopword’ library.

5.	keywords.csv
Keywords that relate to Health Care

6.	phrases.csv
Phrases that that relate to Health Care

7.	postfixes.csv
Postfixes that relate to Health Care

8.	prefixes.csv
Prefixes that relate to Health Care

#### Preparation
Make sure you have a ‘config.json’ file ready in ‘Dictionaries’ folder. If you don’t have a Twitter Dev Account yet, or the ‘config.json’ file doesn’t work properly, please visit [https://dev.twitter.com/][2] to sign up your own Twitter Dev account. Due to security reasons, using your own consumer_key and access_token is highly recommended.

#### Procedures
1.	Get the tweets 
	1.	Run twitterstream.py
	2.	Unless user breaks the operation, the script will run forever to fetch the data from Twitter API. Each Tweet will be extracted as a Json formatted string and the script will output 100 tweets as one huge Json formatted string with comma separated per “time stamped” file.
	3.	Run parseData-v4.py
	4.	Unless user breaks the operation, the script will run forever to parse the Json formatted file from Step 2 to one tweet (Json formatted) per line. Only tweets that match the word in keywords.csv or phrases.csv or postfixes.csv or prefixes.csv will be extracted and output to dump/output.txt. Old “time stamped” file will be deleted after parsed(Only medical related tweets will be extracted and analyzed)
	5.	Note that parseData-v4.py and twitterstream.py can be executed simultaneously

2.	Convert “raw” tweets to “cleaned” tweets
	1.	Run __From_Json_to_Raw_Tweets.py__ This will generate a CSV file with “^” delimited that contains 8 fields for each tweet…Name,Tweet Created Time,Tweet ID,Twitter User Name,User’s followers count,Users friends count,User Defined Time Zone,Tweets Text

3.	Analyze TweetsIf need to analyze NNP words, 
	1.	run From_Raw_to_Cleaned_Tweets_Less.py otherwise, run From_Raw_to_Cleaned_Tweets_Full.py to proceed Matrix making and classifications.

###### Example
1.	Based on the file Tweet Text Processing/cleanedTweetsOnly.csv

1.	NNP
	1.	Run Frm_Raw_to_Cleaned_Tweets_Less.py to clean up tweets without doing lowercase conversion and punctuations removal.
	2.	Run __NER.py__ to generate a list of NNP words with its frequency in ‘processed’ folder

2.Sentiment Analysis
	1.	Sentiment Score Table is based on the file ‘AFINN-111.txt’
	2.	Run __Sentiment_Tweets.py__ to generate a CSV file that has tweets with sentiment scores
	3.	Run __Sentiment_Word_Frequency.py__ to generate a CSV file that has the list of unique words in the tweets set with the frequency counts and sentiment scores
	4.	Run __Word_Freq_Count.py__ to generate a simple list of words with its frequency counts.* or Run wordCount.py to generate s list of words that based on keywords and the other 3 dictionaries that emphasize medical words only, and frequency counts (output folder is ‘processed’)
	5.	Run __Keyword_Filtered_by_Score.py__ to generate keywords that scores higher than a user defined number

3.	Classification
	1.	Run __From_Raw_to_Cleaned_Tweets_Full.py____ __ to make a “cleaned” Tweet file
	2.	Run __From_Tweets_to_matrix_TrainingSet.py __to make a “Training” matrix based on step 3a’s output file.
	3.	Run __Convert_TrainingSetHeaders_To_WordList.py__ to extract a list of word that will be used as our new word list to filter the “Testing” tweets. Backup the old __newWL.csv__ file if desired.
	4.	Run __Remove_By_newWL_get_Tweets.py__ to remove words that not exist in the “Testing” tweets. (Assume that the size of “Training” tweets >> the size of “Testing” tweets.) By doing this, we assure that the “Testing” tweets is the subset of “Training” tweets.
	6.	Run __From_Tweets_to_matrix_TrainingSet.py __to make a “Testing” matrix based on step 3d’s output file.
	7.	Run __Get_Prediction_From_TestingSetMatrix.py __to make a final classification decision.* 
	8.	Run __Xvalidation_Training.py__ to do cross validation (make 20% of the tweets as test set and 80% as train set)Co-occurrenceRun __co-occurrences.py__ to get only keywords’ co-occurrences
	9.	Run __co-occurrences-1-word.py__ to get the specified one word’s  co-occurrences
	10.	Run __co-occurrences-1-word-top-20.py__ to get the sorted top 20 frequent words’ co-occurrences of  step 4b
	11.	Run __co-occurrences-2-word-inersection.py__ to get the specified two word’s co-occurrences__License Grant__

__All scripts in this folder are under GNU General Public License.__


  [1]: http://www2.imm.dtu.dk/pubdb/views/publication_details.php?id=6010
  [2]: https://dev.twitter.com/
