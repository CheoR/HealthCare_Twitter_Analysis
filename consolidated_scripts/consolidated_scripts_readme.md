Health Care Twitter Analysis Public Files Documentation

Edited By: Longbo Qiao

Email: longboqiao@gmail.com

Date:  6/28/2013

#### Installations:

-	Python 2.7.5 is highly recommended.
-	Python libraries you will need(you may need more than this list): ntlk,nose,numpy,oauth,oauth2,scikit_learn,scipy-textmining,tweepy
-	‘Dictionaries’ folder contains all the dictionaries/word list/configuration file

__File Name:__

__Functions:__

AFINN-111.txt

A list of 2477 words and phrases with sentiment scores that defined by Technical University of Denmark. [http://www2.imm.dtu.dk/pubdb/views/publication_details.php?id=6010][1]

config.json

Configuration file that used by __twitterstream.py__ to fetch tweets

newWL.csv

Wordlist that generated by __Generate_newWL.py __for classification

stopwords.csv

A group of less value words in medical context. Has been extracted and modified based on R’s ‘stopword’ library.

keywords.csv

Keywords that relate to Health Care

phrases.csv

Phrases that that relate to Health Care

postfixes.csv

Postfixes that relate to Health Care

prefixes.csv

Prefixes that relate to Health Care

#### Preparation:

Make sure you have a ‘config.json’ file ready in ‘Dictionaries’ folder. If you don’t have a Twitter Dev Account yet, or the ‘config.json’ file doesn’t work properly, please visit [https://dev.twitter.com/][2] to sign up your own Twitter Dev account. Due to security reasons, using your own consumer_key and access_token is highly recommended.

#### Procedures:

Get the tweetsRun __twitterstream.py__Unless user breaks the operation, the script will run forever to fetch the data from Twitter API. Each Tweet will be extracted as a Json formatted string and the script will output 100 tweets as one huge Json formatted string with comma separated per “time stamped” file.Run __parseData-v4.py__Unless user breaks the operation, the script will run forever to parse the Json formatted file from __Step 2__ to one tweet (Json formatted) per line. Only tweets that match the word in __keywords.csv ____/phrases.csv/postfixes.csv/____prefixes.csv will__ be extracted and output to dump/output.txt. Old “time stamped” file will be deleted after parsed(Only medical related tweets will be extracted and analyzed)Note that parseData-v4.py and twitterstream.py can be executed simultaneouslyConvert “raw” tweets to “cleaned” tweetsRun __From_Json_to_Raw_Tweets.py__This will generate a CSV file with “^” delimited that contains 8 fields for each tweet…Name,Tweet Created Time,Tweet ID,Twitter User Name,User’s followers count,Users friends count,User Defined Time Zone,Tweets Text

Analyze TweetsIf need to analyze NNP words, run __Fro____m_Raw_to_Cleaned_Tweets_Less.py;____ __otherwise, run __From_Raw_to_Cleaned_Tweets_Full.py __to proceed Matrix making and classifications.

Based on the file __Tweet Text Processing/cleanedTweetsOnly.csv____…__

NNPRun __Fro____m_Raw_to_Cleaned_Tweets_Less.py __to clean up tweets without doing lowercase conversion and punctuations removal.Run __NER.py__ to generate a list of NNP words with its frequency in ‘processed’ folderSentiment AnalysisSentiment Score Table is based on the file ‘AFINN-111.txt’

Run __Sentiment_Tweets.py__ to generate a CSV file that has tweets with sentiment scoresRun __Sentiment_Word_Frequency.py__ to generate a CSV file that has the list of unique words in the tweets set with the frequency counts and sentiment scoresRun __Word_Freq_Count.py__ to generate a simple list of words with its frequency counts.* or Run wordCount.py to generate s list of words that based on keywords and the other 3 dictionaries that emphasize medical words only, and frequency counts (output folder is ‘processed’)Run __Keyword_Filtered_by_Score.py__ to generate keywords that scores higher than a user defined numberClassificationRun __From_Raw_to_Cleaned_Tweets_Full.py____ __ to make a “cleaned” Tweet fileRun __From_Tweets_to_matrix_TrainingSet.py __to make a “Training” matrix based on step 3a’s output file.Run __Convert_TrainingSetHeaders_To_WordList.py__ to extract a list of word that will be used as our new word list to filter the “Testing” tweets. Backup the old __newWL.csv__ file if desired.Run __Remove_By_newWL_get_Tweets.py__ to remove words that not exist in the “Testing” tweets. (Assume that the size of “Training” tweets >> the size of “Testing” tweets.) By doing this, we assure that the “Testing” tweets is the subset of “Training” tweets.Run __From_Tweets_to_matrix_TrainingSet.py __to make a “Testing” matrix based on step 3d’s output file.Run __Get_Prediction_From_TestingSetMatrix.py __to make a final classification decision.* Run __Xvalidation_Training.py__ to do cross validation (make 20% of the tweets as test set and 80% as train set)Co-occurrenceRun __co-occurrences.py__ to get only keywords’ co-occurrencesRun __co-occurrences-1-word.py__ to get the specified one word’s  co-occurrencesRun __co-occurrences-1-word-top-20.py__ to get the sorted top 20 frequent words’ co-occurrences of  step 4bRun __co-occurrences-2-word-inersection.py__ to get the specified two word’s co-occurrences__License Grant__

__All scripts in this folder are under GNU General Public License.__


  [1]: http://www2.imm.dtu.dk/pubdb/views/publication_details.php?id=6010
  [2]: https://dev.twitter.com/
